import streamlit as st
import time
import psutil
import os
import mlx.core as mx  # ç”¨äºç›‘æ§ Apple Silicon æ˜¾å­˜
from mlx_lm import load, stream_generate

st.set_page_config(page_title="MLX M4 LLM æœåŠ¡å™¨", page_icon="âš¡ï¸")
st.title("âš¡ï¸ MLX M4 LLM æ¨ç†æœåŠ¡å™¨")
st.sidebar.header("âš™ï¸ åŠ©æ‰‹é…ç½®")
new_sys_prompt = st.sidebar.text_area("ç³»ç»Ÿæç¤ºè¯ (System Prompt)", value="ä½ æ˜¯ä¸€ä¸ªå‡†ç¡®ã€ä¸“ä¸šä¸”ç®€æ´çš„ AI åŠ©æ‰‹ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚")

st.sidebar.header("ğŸ“ˆ å®æ—¶æ€§èƒ½æŒ‡æ ‡")
ttft_metric = st.sidebar.empty() # é¦–ä¸ª Token å»¶è¿Ÿ
tps_metric = st.sidebar.empty()  # ç”Ÿæˆé€Ÿåº¦ (token/s)
token_count_metric = st.sidebar.empty()  # å·²ç”Ÿæˆçš„ Token æ•°

def get_process_memory():
    """è·å–å½“å‰è¿›ç¨‹å ç”¨çš„ç‰©ç†å†…å­˜ (GB)"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / (1024 ** 3)

# 1. è®°å½•åº”ç”¨å¯åŠ¨æ—¶çš„åŸºå‡†å†…å­˜
if "baseline_mem" not in st.session_state:
    st.session_state.baseline_mem = get_process_memory()

# å¸¦æœ‰å†…å­˜è¿½è¸ªçš„æ¨¡å‹åŠ è½½å‡½æ•°
@st.cache_resource
def init_model():
    # 1. è®°å½•åº”ç”¨å¯åŠ¨åŸºå‡†
    baseline = get_process_memory()

    model_path = "models/qwen2.5-14b-instruct-bits-8"
    model, tokenizer = load(model_path, {"lazy": True})

    # å¼ºåˆ¶è¯„ä¼°æ‰€æœ‰å‚æ•°ï¼Œç¡®ä¿æƒé‡çœŸæ­£å ç”¨æ˜¾å­˜ï¼Œå¦åˆ™æƒé‡ä¼šè¢«è®¡å…¥åç»­çš„ KV Cache ä¸­
    mx.eval(model.parameters())

    # 2. æ­¤æ—¶è·å–çš„æ´»åŠ¨æ˜¾å­˜å³ä¸ºâ€œçº¯é™æ€æƒé‡â€ï¼Œä½¿ç”¨æ¨èçš„æ–°ç‰ˆ API: mx.get_active_memory()
    total_active_now = mx.get_active_memory() / (1024 ** 3)

    return model, tokenizer, total_active_now

model, tokenizer, model_weight_mem = init_model()

# --- ä¾§è¾¹æ è®¾ç½® (å†…å­˜æŒ‡æ ‡æ•´åˆ) ---
st.sidebar.header("ğŸ“Š å†…å­˜åŠ¨æ€ç›‘æ§")
# æŒ‰è¡Œåˆ›å»ºå®¹å™¨
with st.sidebar.container():
    sys_mem_metric = st.sidebar.empty()    # ç¬¬ä¸€è¡Œï¼šç³»ç»Ÿæ€»å ç”¨
    mlx_active_metric = st.sidebar.empty() # ç¬¬äºŒè¡Œï¼šMLX æ´»åŠ¨æ˜¾å­˜
    cache_mem_metric = st.sidebar.empty()  # ç¬¬ä¸‰è¡Œï¼šKV Cache å ç”¨
    st.sidebar.divider()

# --- åŠ¨æ€æ›´æ–°å‡½æ•° ---
# ç³»ç»Ÿè¿›ç¨‹å†…å­˜ (RSS) â‰ˆ æ´»åŠ¨æ˜¾å­˜ (Active Memory) + åº”ç”¨ç¨‹åºå¼€é”€ (Python/Streamlit)
# æ´»åŠ¨æ˜¾å­˜ (Active Memory) = æ¨¡å‹é™æ€æƒé‡ (Weights) + åŠ¨æ€ç¼“å­˜ (KV Cache)
def update_metrics(token_count):
    # A. è·å–å½“å‰ MLX æ€»æ´»åŠ¨å†…å­˜ (GB)
    # åŒ…å«ï¼šé™æ€æƒé‡ + åŠ¨æ€ KV Cache
    current_active_gb = mx.get_active_memory() / (1024 ** 3)

    # B. è®¡ç®— KV Cache (GB)
    # ç°åœ¨çš„é€»è¾‘æ˜¯ï¼š(æƒé‡ + Cache) - æƒé‡ = Cache
    kv_cache_gb = max(0, current_active_gb - model_weight_mem)

    # C. è·å–ç³»ç»Ÿè¿›ç¨‹å†…å­˜ (ä½œä¸ºå‚è€ƒ)
    current_rss = get_process_memory()

    ########################################################################
    ### â€œç³»ç»Ÿè¿›ç¨‹å†…å­˜ (RSS)â€ ä¼šè¿œå°äºâ€œæ´»åŠ¨æ˜¾å­˜â€ã€‚è¿™æ˜¯å› ä¸º Apple Metal æ¡†æ¶åˆ†é…çš„å†…å­˜
    ### ä¼šè¢«å½’ç±»ä¸ºâ€œç³»ç»Ÿé©±åŠ¨å±‚å ç”¨â€ï¼Œè€Œä¸å®Œå…¨è®¡å…¥â€œç”¨æˆ·è¿›ç¨‹ç§æœ‰å ç”¨â€ã€‚
    ########################################################################
    # 1. ç³»ç»Ÿè¿›ç¨‹å†…å­˜ï¼šåæ˜  macOS æŠ¥å‘Šçš„ç‰©ç†å ç”¨
    sys_mem_metric.metric("ğŸ–¥ï¸ ç³»ç»Ÿè¿›ç¨‹å†…å­˜ (RSS)", f"{current_rss:.3f} GB")

    # 2. æ´»åŠ¨æ˜¾å­˜ï¼šåæ˜  GPU å®é™…é”å®šçš„ç»Ÿä¸€å†…å­˜ï¼ˆæœ€å‡†ï¼‰
    mlx_active_metric.metric("ğŸ§  å®é™…æ´»åŠ¨æ˜¾å­˜", f"{current_active_gb:.3f} GB")

    # 3. KV Cacheï¼šæ˜¾ç¤ºé«˜ç²¾åº¦æ•°å€¼æˆ–è½¬ä¸º MB
    if kv_cache_gb < 0.5:
        cache_mem_metric.metric("ğŸŒ€ KV Cache å ç”¨", f"{kv_cache_gb * 1024:.2f} MB")
    else:
        cache_mem_metric.metric("ğŸŒ€ KV Cache å ç”¨", f"{kv_cache_gb:.4f} GB")

    token_count_metric.metric("ğŸ”¢ å·²ç”Ÿæˆè®¡æ•°", f"{token_count} tokens")

# --- èŠå¤©é€»è¾‘ ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": new_sys_prompt}]
else:
    st.session_state.messages[0]["content"] = new_sys_prompt

for message in st.session_state.messages:
    if message["role"] != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

if prompt := st.chat_input("åœ¨æ­¤è¾“å…¥æ¶ˆæ¯..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        # æ„å»ºä¸Šä¸‹æ–‡
        formatted_prompt = tokenizer.apply_chat_template(
            st.session_state.messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # æ€§èƒ½è¿½è¸ªåˆå§‹åŒ–
        start_time = time.time()
        ttft = None
        token_count = 0

        # --- æµå¼ç”Ÿæˆå¾ªç¯ ---
        for response in stream_generate(model, tokenizer, prompt=formatted_prompt, max_tokens=1000):
            # 1. å¤„ç†å“åº”å¯¹è±¡ (å…¼å®¹ä¸åŒ MLX ç‰ˆæœ¬)
            delta_text = response.text if hasattr(response, 'text') else response
            
            full_response += delta_text
            token_count += 1
            
            # 2. è®°å½•é¦–ä¸ª Token åˆ°è¾¾æ—¶é—´ (TTFT)
            if ttft is None and delta_text.strip() != "":
                ttft = time.time() - start_time
                generation_start_time = time.time() # ä»è¿™ä¸€åˆ»å¼€å§‹ç®— TPS
                ttft_metric.metric("ğŸš€ é¦–å­—å»¶è¿Ÿ", f"{ttft*1000:.0f} ms")

            # 3. è®¡ç®—ç”Ÿæˆé€Ÿåº¦ (TPS)
            if generation_start_time:
                t_elapsed = time.time() - generation_start_time
                if t_elapsed > 0:
                    tps_metric.metric("âš¡ï¸ ç”Ÿæˆé€Ÿåº¦", f"{token_count / t_elapsed:.2f} t/s")

            # 4. æ›´æ–° Token ç»Ÿè®¡
            token_count_metric.metric("å·²ç”Ÿæˆ Token", f"{token_count} tokens")
            
            # 5. å®æ—¶æ›´æ–°ä¾§è¾¹æ å†…å­˜æ•°æ®
            update_metrics(token_count)

            # 6. æ›´æ–° UI æ¸²æŸ“
            response_placeholder.markdown(full_response + "â–Œ")
        
        response_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
